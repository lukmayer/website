---
description: "Lukas Mayer's research in computational cognitive science, Human-AI collaboration. Related topics include causal reasoning, inversion problems, AI alignment, data-science, software development"

listing:
  id: publications
  template: publication.ejs
  contents:
    - publications.yml
  sort-ui: false
  filter-ui: false
  page-size: 100
  field-display-names:
    title: "Title"
    journal: "Journal"
    authors: "Authors"
    image: "Image"
    osf: "OSF"
    paper: "Paper"
    paragraph-header: "Header"
    descriptive-paragraph: "Description"

resources: assets/cogsci25_poster.pdf

format: 
  html:
    toc: false
    callout-appearance: default
    code-overflow: wrap
    code-line-numbers: true
    code-block-border-left: false
---

::: {.column-page-inset-left}


<style>

.listing-descriptive-paragraph {
  transition: max-height 0.5s ease;
  overflow: hidden; /* Helps in controlling the transition */
}

.button-link {
  transition: transform 0.3s ease;
}

.button-link:hover {
  transform: scale(1.1);
}


/* Remove bullet points from the list */
.pub-list {
  list-style: none;
  padding: 0;
  margin: 0;
}

/* Set the pub-item as a flex container with vertical centering */
.pub-item {
  display: flex;
  gap: 20px;            
}

.responsive-pub-image {
  max-width: 100%;
  height: auto;
  border-radius: 0px;
  border: 1px solid #333;
  width: auto;
}

/* Responsive breakpoints */
@media (max-width: 768px) {
  .responsive-pub-image {
    max-width: 90%;
  }
}

@media (max-width: 480px) {
  .responsive-pub-image {
    max-width: 95%;
  }
}


/* Let the details container fill the remaining space */
.pub-details {
  flex: 1;
}
.highlight-author {
  font-weight: bold;
}

.special-header {
  font-weight: bold;
}

.listing-journal {
  font-weight: italic;
}


.description {
    display: none;
}

@media (max-width: 768px) {
  .pub-item {
    flex-direction: column;  
    gap: 15px;               
  }

  /* Adjust the image container  */
  .pub-image img {
    max-width: 80%; 
  }


}

.small-break {
  line-height: 0.1;
  font-size: 0.5em;
}


</style>


# Publications



## Published


::: {#publications}
:::


::: text-center

\

---


::: {.columns}

::: {.column width=50%}
<br class="small-break">
{{< ai google-scholar size=huge >}} [More on Google Scholar](https://scholar.google.com/citations?user=KsH37lMAAAAJ)
:::

::: {.column width=50%}
<br class="small-break">
{{< ai semantic-scholar size=huge >}} [More on Semantic Scholar](https://www.semanticscholar.org/author/Lukas-William-Mayer/2281033339)
:::

::: 
<!-- end columns -->

---

\

:::
<!-- end text-center -->

## Posters and Talks


::::: {.callout-important collapse="true" icon="false"}
# ***"A Cognitive Framework for Strategic AI Communication"***
**Mayer, L. W.**, Steyvers, M. *CogSci 2025*

::: text-center
<iframe src="assets/cogsci25_poster.pdf#toolbar=0&navpanes=0&scrollbar=0&view=FitH&page=1" style="width: 90%; height: 1000px; border: none;"></iframe>
:::

:::::


\


## In Progress


::::: {.callout-important collapse="true" icon="false"}
# ***"Human-AI Collaboration: Trade-offs Between Performance and Preferences"*** 
**Mayer, L. W.**, Karny, S., Ayoub, J., Song, M., Tian, D., Moradi-Pari, E., Steyvers, M. (under Review) *Cognitive Research: Principles and Implications*

::: text-center
![](assets/collab_ai.png)
:::

\

**How can we make AI collaborate well with people?** Narrowly optimizing the performance of AI agents may be convenient, but can cause frustration when people are then asked to work with this agent.
In this paper, we show that people prefer AI agents that are considerate of their preferences, even when this comes at the cost of performance.
We also find that certain human-centric design choices boost people's liking of the agent, without harming the performance of the human-AI team.
Our results strongly suggest that leveraging both subjective and objective metrics is crucial when designing AI agents for human collaboration.

::: text-center
[OSF repository](https://osf.io/ybweq/?view_only=cb4d4c7ac0b848b79b6ae8c7b09278cc)
:::
:::::

\



<script>
document.addEventListener('DOMContentLoaded', function() {
  var myName = "Mayer, L. W.";
  var authorElements = document.querySelectorAll('.authors-string');
  var regex = new RegExp(myName, 'g');
  authorElements.forEach(function(elem) {
    elem.innerHTML = elem.innerHTML.replace(
      regex,
      '<span class="highlight-author">' + myName + '</span>'
    );
  });

</script>

<script>
document.addEventListener('DOMContentLoaded', function() {
  document.querySelectorAll('.callout-title-container').forEach(function(container) {
    container.addEventListener('click', function() {
      const index = this.getAttribute('data-index');
      const element = document.getElementById('pub-' + index);
      const arrow = this.querySelector('.toggle-arrow');

      if (element.style.display === 'none') {
        element.style.display = 'block';
        arrow.innerHTML = '▲';
      } else {
        element.style.display = 'none';
        arrow.innerHTML = '▼';
      }
    });
  });
});
</script>

\


# Research Interests

At heart, I study human decision-making. I have a particular interest in using Cognitive Models as a means of specifying the causal processes that give rise to people's choices. Human-AI collaboration has become an extremely convenient outlet for me in this regard. For one, contemporary AI technologies' applications in the real world offer plenty inspiration for my research. For example, I'm interested in how people trade-off the costs of outsourcing effortful cognitive labor to a computational agent, and how far you can push the attributes of this agent before eliciting a behavioral shift in the participant. Moreover, because Cognitive Models are mathematical in nature, casting my research through a lens of human-AI collaboration has immediate practical implications as solutions to "misaligned" behaviors are more readily generated from formal explanations of human behavior. In my research, the "AI" primarily serves as a theoretical tool to ask questions like: «How would people behave if they had access to an agent with X, Y properties?». That being said, I have a definitive interest in demonstrating the applied value of my work, meaning that some of my projects go the extra length of combining psychological models with algorithms from AI research. 

The project that currently enjoys most of my attention intentionally pairs participants with an incredibly annoying to use "AI" assistant to understand how the timing of AI-induced interruptions influences the participant's motivation to permanently disable this agent. With a causal model of people's annoyance decisions integrated, our simulations show it should be possible to estimate policies for the AI assistant that avoid interrupting the participant at times where advice is unlikely to be appreciated. Such policies could lead to much more sustainable and productive collaboration. 





\




:::
